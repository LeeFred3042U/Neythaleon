# Neythaleon

**Neythaleon** is an efficient, dependency-light data ingestion and observability toolkit for marine biodiversity datasets. Originally built for IndOBIS datasets, it ingests `.parquet` files, performs a robust cleaning pipeline, streams the data to PostgreSQL, and tracks detailed system metrics.

> _"The Eye Below Logs Everything."_

---

## Features

- ðŸ  **Efficient Ingestion:** Ingests large `.parquet` files in memory-safe chunks using DuckDB.
- âš™ï¸ **Robust ETL:** A multi-step cleaning and transformation pipeline handles data types, special characters, and complex geometry formats.
- ðŸ˜ **High-Performance Loading:** Uses PostgreSQL's native `COPY` command for fast, bulk data insertion.
- ðŸ“ˆ **Comprehensive Observability:** Logs detailed performance metrics, including CPU, RAM, throughput, and processing time per batch.
- ðŸ›¡ï¸ **Schema Safe:** Automatically synchronizes the data's structure with the database table schema to prevent load failures.
- ðŸ“¦ **Automated Setup:** Can automatically create the target database table if it doesn't exist.

---

## ðŸ§  Architecture

1.  **Extract:** Read `.parquet` files in memory-efficient chunks using DuckDB.
2.  **Transform:** A pipeline of cleaning functions is applied to each chunk:
    * Drop sparse columns and rows without valid coordinates.
    * Enforce correct integer data types.
    * Sanitize text fields by removing special characters.
    * Convert binary geometry (WKB) to text (WKT).
3.  **Align:** The schema of the cleaned data is dynamically matched to the target database table's schema.
4.  **Load:** The prepared data is bulk-loaded into PostgreSQL via an in-memory TSV stream.
5.  **Log:**
    * Time taken, CPU %, and RAM % are recorded for each chunk.
    * Rows ingested and processing speed (rows/sec) are calculated.
    * Metrics are exported to `metrics_log.csv` and human-readable logs.

---

## ðŸ“Š Sample Visualizations

> These visualizations are generated from the `metrics_log.csv` file, providing a clear view of the pipeline's performance under load.
> Run `python plot.py` to generate these visuals in the `media/` directory.

### â±ï¸ Ingestion Throughput
![Ingestion Throughput](media/throughput_report.png)
Shows rows ingested per second, with a rolling average to highlight performance stability and trends over the entire run.

### ðŸ§  CPU vs. Throughput
![CPU vs Throughput](media/cpu_vs_throughput_report.png)
This scatter plot reveals how CPU load correlates with ingestion speed, helping to identify performance sweet spots and bottlenecks.

### ðŸ’¾ Memory Usage
![Memory Usage](media/memory_usage_report.png)
Tracks memory consumption over time, verifying the stability and efficiency of the streaming process.

### â³ Batch Processing Time
![Batch Processing Time](media/batch_time_report.png)
A bar chart showing the time taken to process each individual batch, making it easy to spot outliers and inconsistent performance.

---

## ðŸ“ File Structure

```bash
.
â”œâ”€â”€ ingest.py              # Main pipeline script
â”œâ”€â”€ plot.py                # Generates performance dashboards
â”œâ”€â”€ metrics_log.csv        # System + ingestion metrics (autogenerated)
â”œâ”€â”€ ingestion.log          # Step-by-step logs (autogenerated)
â”œâ”€â”€ .env                   # Database credentials (e.g., DATABASE_URL)
â”œâ”€â”€ requirements.txt       
â””â”€â”€ parquet_files/         # Directory for source .parquet files
```
---
## ðŸ”§ Requirements
- Python 3.8+

- DuckDB

- Pandas

- SQLAlchemy

- Shapely

- Psycopg2 (or other DB driver)

- A PostgreSQL-compatible database

### Install via:
```bash
pip install -r requirements.txt
```

---

## ðŸ™ Dataset
This project uses records from OBIS - the Ocean Biodiversity Information System. The data represents a full dump from July 2025. Neythaleon is designed to handle the raw complexity of this data, performing the necessary preprocessing to make it database-ready.

### Source:
OBIS Data Portal (July 2025)

### **Citation**:

OBIS (2025). Global distribution records from the OBIS database.
Ocean Biodiversity Information System. Intergovernmental Oceanographic Commission of UNESCO.

Available at: [OBIS](https://obis.org/data/access/).

---

## â„ï¸ Why This Matters

Building observable and reproducible data pipelines is fundamental to reliable data science. This project serves as a real-world example of analyzing and optimizing a Python ETL script, demonstrating how to identify and resolve complex performance bottlenecks through iterative testing and analysis.

---

## ðŸªª License
- Code: MIT

- Data: CC0 1.0 

--- 
 
## âš“ Quote That Hit Different
> "The sea, once it casts its spell, holds one in its net of wonder forever."
   â€” Jacques Cousteau
---